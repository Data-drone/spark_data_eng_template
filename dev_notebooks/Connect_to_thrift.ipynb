{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "behind-brown",
   "metadata": {},
   "source": [
    "# Notebook for testing the thrift server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-creation",
   "metadata": {},
   "source": [
    "verify pyhive native connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "waiting-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "connection_string = 'spark-thrift-server:10000'\n",
    "connection = hive.connect(username='root', host=\"spark-thrift-server\", port=10000, auth='NOSASL')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SHOW DATABASES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "color-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('clean',)\n",
      "('default',)\n"
     ]
    }
   ],
   "source": [
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stainless-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('clean', 'green_clean', False)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('SHOW TABLES in clean')\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hired-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('default', 'green_merged', False)\n",
      "('default', 'green_taxi_2015_h1', False)\n",
      "('default', 'green_taxi_pre2015', False)\n",
      "('default', 'yellow_taxi_pre2015', False)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('SHOW TABLES in default')\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "starting-lightweight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Database Name', 'clean')\n",
      "('Comment', '')\n",
      "('Location', 's3a://storage/warehouse/clean')\n",
      "('Owner', 'root')\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('DESCRIBE DATABASE clean')\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entitled-relations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('VendorID', 'string', '')\n",
      "('lpep_pickup_datetime', 'string', '')\n",
      "('Lpep_dropoff_datetime', 'string', '')\n",
      "('Store_and_fwd_flag', 'string', '')\n",
      "('RateCodeID', 'string', '')\n",
      "('Pickup_longitude', 'string', '')\n",
      "('Pickup_latitude', 'string', '')\n",
      "('Dropoff_longitude', 'string', '')\n",
      "('Dropoff_latitude', 'string', '')\n",
      "('Passenger_count', 'string', '')\n",
      "('Trip_distance', 'string', '')\n",
      "('Fare_amount', 'string', '')\n",
      "('Extra', 'string', '')\n",
      "('MTA_tax', 'string', '')\n",
      "('Tip_amount', 'string', '')\n",
      "('Tolls_amount', 'string', '')\n",
      "('Ehail_fee', 'string', '')\n",
      "('Total_amount', 'string', '')\n",
      "('Payment_type', 'string', '')\n",
      "('trip_type', 'string', '')\n",
      "('improvement_surcharge', 'string', '')\n",
      "('', '', '')\n",
      "('# Partitioning', '', '')\n",
      "('Not partitioned', '', '')\n"
     ]
    }
   ],
   "source": [
    "#cursor.execute(\"USE default\")\n",
    "cursor.execute('DESCRIBE TABLE default.green_merged')\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ancient-custom",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: java.lang.NullPointerException:36:35', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:361', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:249', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:278', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:44', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:78', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:62', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:44', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:42', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:43', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'sun.reflect.GeneratedMethodAccessor53:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1730', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy26:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:457', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:53', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*java.lang.NullPointerException:null:132:96', 'org.apache.spark.util.ClosureCleaner$:clean:ClosureCleaner.scala:359', 'org.apache.spark.util.ClosureCleaner$:clean:ClosureCleaner.scala:162', 'org.apache.spark.SparkContext:clean:SparkContext.scala:2465', 'org.apache.spark.rdd.RDD:$anonfun$map$1:RDD.scala:422', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:151', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:112', 'org.apache.spark.rdd.RDD:withScope:RDD.scala:414', 'org.apache.spark.rdd.RDD:map:RDD.scala:421', 'org.apache.spark.sql.delta.util.StateCache$CachedDS:<init>:StateCache.scala:52', 'org.apache.spark.sql.delta.util.StateCache:cacheDS:StateCache.scala:100', 'org.apache.spark.sql.delta.util.StateCache:cacheDS$:StateCache.scala:99', 'org.apache.spark.sql.delta.Snapshot:cacheDS:Snapshot.scala:55', 'org.apache.spark.sql.delta.Snapshot:cachedState$lzycompute:Snapshot.scala:117', 'org.apache.spark.sql.delta.Snapshot:cachedState:Snapshot.scala:116', 'org.apache.spark.sql.delta.Snapshot:state:Snapshot.scala:120', 'org.apache.spark.sql.delta.Snapshot:$anonfun$computedState$1:Snapshot.scala:140', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withJobDescription:DeltaProgressReporter.scala:53', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withStatusCode:DeltaProgressReporter.scala:32', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withStatusCode$:DeltaProgressReporter.scala:27', 'org.apache.spark.sql.delta.Snapshot:withStatusCode:Snapshot.scala:55', 'org.apache.spark.sql.delta.Snapshot:computedState$lzycompute:Snapshot.scala:137', 'org.apache.spark.sql.delta.Snapshot:computedState:Snapshot.scala:136', 'org.apache.spark.sql.delta.Snapshot:metadata:Snapshot.scala:179', 'org.apache.spark.sql.delta.Snapshot:toString:Snapshot.scala:290', 'java.lang.String:valueOf:String.java:2994', 'java.lang.StringBuilder:append:StringBuilder.java:131', 'org.apache.spark.sql.delta.Snapshot:$anonfun$new$1:Snapshot.scala:293', 'org.apache.spark.sql.delta.Snapshot:$anonfun$logInfo$1:Snapshot.scala:270', 'org.apache.spark.internal.Logging:logInfo:Logging.scala:57', 'org.apache.spark.internal.Logging:logInfo$:Logging.scala:56', 'org.apache.spark.sql.delta.Snapshot:logInfo:Snapshot.scala:270', 'org.apache.spark.sql.delta.Snapshot:<init>:Snapshot.scala:293', 'org.apache.spark.sql.delta.SnapshotManagement:createSnapshot:SnapshotManagement.scala:223', 'org.apache.spark.sql.delta.SnapshotManagement:createSnapshot$:SnapshotManagement.scala:211', 'org.apache.spark.sql.delta.DeltaLog:createSnapshot:DeltaLog.scala:59', 'org.apache.spark.sql.delta.SnapshotManagement:getSnapshotAtInit:SnapshotManagement.scala:195', 'org.apache.spark.sql.delta.SnapshotManagement:getSnapshotAtInit$:SnapshotManagement.scala:186', 'org.apache.spark.sql.delta.DeltaLog:getSnapshotAtInit:DeltaLog.scala:59', 'org.apache.spark.sql.delta.SnapshotManagement:$init$:SnapshotManagement.scala:49', 'org.apache.spark.sql.delta.DeltaLog:<init>:DeltaLog.scala:63', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$3:DeltaLog.scala:467', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$2:DeltaLog.scala:467', 'com.databricks.spark.util.DatabricksLogging:recordOperation:DatabricksLogging.scala:77', 'com.databricks.spark.util.DatabricksLogging:recordOperation$:DatabricksLogging.scala:67', 'org.apache.spark.sql.delta.DeltaLog$:recordOperation:DeltaLog.scala:367', 'org.apache.spark.sql.delta.metering.DeltaLogging:recordDeltaOperation:DeltaLogging.scala:106', 'org.apache.spark.sql.delta.metering.DeltaLogging:recordDeltaOperation$:DeltaLogging.scala:91', 'org.apache.spark.sql.delta.DeltaLog$:recordDeltaOperation:DeltaLog.scala:367', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$1:DeltaLog.scala:466', 'com.google.common.cache.LocalCache$LocalManualCache$1:load:LocalCache.java:4792', 'com.google.common.cache.LocalCache$LoadingValueReference:loadFuture:LocalCache.java:3599', 'com.google.common.cache.LocalCache$Segment:loadSync:LocalCache.java:2379', 'com.google.common.cache.LocalCache$Segment:lockedGetOrLoad:LocalCache.java:2342', 'com.google.common.cache.LocalCache$Segment:get:LocalCache.java:2257', 'com.google.common.cache.LocalCache:get:LocalCache.java:4000', 'com.google.common.cache.LocalCache$LocalManualCache:get:LocalCache.java:4789', 'org.apache.spark.sql.delta.DeltaLog$:apply:DeltaLog.scala:464', 'org.apache.spark.sql.delta.DeltaLog$:forTable:DeltaLog.scala:401', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:deltaLog$lzycompute:DeltaTableV2.scala:73', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:deltaLog:DeltaTableV2.scala:73', 'org.apache.spark.sql.delta.DeltaUnsupportedOperationsCheck:$anonfun$apply$1:DeltaUnsupportedOperationsCheck.scala:119', 'org.apache.spark.sql.delta.DeltaUnsupportedOperationsCheck:$anonfun$apply$1$adapted:DeltaUnsupportedOperationsCheck.scala:52', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreach:TreeNode.scala:173', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreach$1:TreeNode.scala:174', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreach$1$adapted:TreeNode.scala:174', 'scala.collection.immutable.List:foreach:List.scala:392', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreach:TreeNode.scala:174', 'org.apache.spark.sql.delta.DeltaUnsupportedOperationsCheck:apply:DeltaUnsupportedOperationsCheck.scala:52', 'org.apache.spark.sql.delta.DeltaUnsupportedOperationsCheck:apply:DeltaUnsupportedOperationsCheck.scala:36', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis$46:CheckAnalysis.scala:699', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis$46$adapted:CheckAnalysis.scala:699', 'scala.collection.mutable.ResizableArray:foreach:ResizableArray.scala:62', 'scala.collection.mutable.ResizableArray:foreach$:ResizableArray.scala:55', 'scala.collection.mutable.ArrayBuffer:foreach:ArrayBuffer.scala:49', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis:CheckAnalysis.scala:699', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis$:CheckAnalysis.scala:90', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis:Analyzer.scala:155', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:176', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:228', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:173', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:73', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:111', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:143', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:143', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:73', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:71', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:63', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:98', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:96', 'org.apache.spark.sql.SparkSession:$anonfun$sql$1:SparkSession.scala:615', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:610', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:650', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:325'], sqlState=None, errorCode=0, errorMessage='Error running query: java.lang.NullPointerException'), operationHandle=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-de6200adc419>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cursor.execute(\"USE default\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DESCRIBE TABLE clean.green_clean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/spark/lib/python3.8/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecuteStatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0m_check_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operationHandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperationHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/spark/lib/python3.8/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusCode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTStatusCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: java.lang.NullPointerException:36:35', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:361', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:249', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:278', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:44', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:78', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:62', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:44', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:42', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:43', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'sun.reflect.GeneratedMethodAccessor53:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1730', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy26:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:457', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:53', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*java.lang.NullPointerException:null:132:96', 'org.apache.spark.util.ClosureCleaner$:clean:ClosureCleaner.scala:359', 'org.apache.spark.util.ClosureCleaner$:clean:ClosureCleaner.scala:162', 'org.apache.spark.SparkContext:clean:SparkContext.scala:2465', 'org.apache.spark.rdd.RDD:$anonfun$map$1:RDD.scala:422', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:151', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:112', 'org.apache.spark.rdd.RDD:withScope:RDD.scala:414', 'org.apache.spark.rdd.RDD:map:RDD.scala:421', 'org.apache.spark.sql.delta.util.StateCache$CachedDS:<init>:StateCache.scala:52', 'org.apache.spark.sql.delta.util.StateCache:cacheDS:StateCache.scala:100', 'org.apache.spark.sql.delta.util.StateCache:cacheDS$:StateCache.scala:99', 'org.apache.spark.sql.delta.Snapshot:cacheDS:Snapshot.scala:55', 'org.apache.spark.sql.delta.Snapshot:cachedState$lzycompute:Snapshot.scala:117', 'org.apache.spark.sql.delta.Snapshot:cachedState:Snapshot.scala:116', 'org.apache.spark.sql.delta.Snapshot:state:Snapshot.scala:120', 'org.apache.spark.sql.delta.Snapshot:$anonfun$computedState$1:Snapshot.scala:140', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withJobDescription:DeltaProgressReporter.scala:53', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withStatusCode:DeltaProgressReporter.scala:32', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withStatusCode$:DeltaProgressReporter.scala:27', 'org.apache.spark.sql.delta.Snapshot:withStatusCode:Snapshot.scala:55', 'org.apache.spark.sql.delta.Snapshot:computedState$lzycompute:Snapshot.scala:137', 'org.apache.spark.sql.delta.Snapshot:computedState:Snapshot.scala:136', 'org.apache.spark.sql.delta.Snapshot:metadata:Snapshot.scala:179', 'org.apache.spark.sql.delta.Snapshot:toString:Snapshot.scala:290', 'java.lang.String:valueOf:String.java:2994', 'java.lang.StringBuilder:append:StringBuilder.java:131', 'org.apache.spark.sql.delta.Snapshot:$anonfun$new$1:Snapshot.scala:293', 'org.apache.spark.sql.delta.Snapshot:$anonfun$logInfo$1:Snapshot.scala:270', 'org.apache.spark.internal.Logging:logInfo:Logging.scala:57', 'org.apache.spark.internal.Logging:logInfo$:Logging.scala:56', 'org.apache.spark.sql.delta.Snapshot:logInfo:Snapshot.scala:270', 'org.apache.spark.sql.delta.Snapshot:<init>:Snapshot.scala:293', 'org.apache.spark.sql.delta.SnapshotManagement:createSnapshot:SnapshotManagement.scala:223', 'org.apache.spark.sql.delta.SnapshotManagement:createSnapshot$:SnapshotManagement.scala:211', 'org.apache.spark.sql.delta.DeltaLog:createSnapshot:DeltaLog.scala:59', 'org.apache.spark.sql.delta.SnapshotManagement:getSnapshotAtInit:SnapshotManagement.scala:195', 'org.apache.spark.sql.delta.SnapshotManagement:getSnapshotAtInit$:SnapshotManagement.scala:186', 'org.apache.spark.sql.delta.DeltaLog:getSnapshotAtInit:DeltaLog.scala:59', 'org.apache.spark.sql.delta.SnapshotManagement:$init$:SnapshotManagement.scala:49', 'org.apache.spark.sql.delta.DeltaLog:<init>:DeltaLog.scala:63', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$3:DeltaLog.scala:467', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$2:DeltaLog.scala:467', 'com.databricks.spark.util.DatabricksLogging:recordOperation:DatabricksLogging.scala:77', 'com.databricks.spark.util.DatabricksLogging:recordOperation$:DatabricksLogging.scala:67', 'org.apache.spark.sql.delta.DeltaLog$:recordOperation:DeltaLog.scala:367', 'org.apache.spark.sql.delta.metering.DeltaLogging:recordDeltaOperation:DeltaLogging.scala:106', 'org.apache.spark.sql.delta.metering.DeltaLogging:recordDeltaOperation$:DeltaLogging.scala:91', 'org.apache.spark.sql.delta.DeltaLog$:recordDeltaOperation:DeltaLog.scala:367', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$1:DeltaLog.scala:466', 'com.google.common.cache.LocalCache$LocalManualCache$1:load:LocalCache.java:4792', 'com.google.common.cache.LocalCache$LoadingValueReference:loadFuture:LocalCache.java:3599', 'com.google.common.cache.LocalCache$Segment:loadSync:LocalCache.java:2379', 'com.google.common.cache.LocalCache$Segment:lockedGetOrLoad:LocalCache.java:2342', 'com.google.common.cache.LocalCache$Segment:get:LocalCache.java:2257', 'com.google.common.cache.LocalCache:get:LocalCache.java:4000', 'com.google.common.cache.LocalCache$LocalManualCache:get:LocalCache.java:4789', 'org.apache.spark.sql.delta.DeltaLog$:apply:DeltaLog.scala:464', 'org.apache.spark.sql.delta.DeltaLog$:forTable:DeltaLog.scala:401', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:deltaLog$lzycompute:DeltaTableV2.scala:73', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:deltaLog:DeltaTableV2.scala:73', 'org.apache.spark.sql.delta.DeltaUnsupportedOperationsCheck:$anonfun$apply$1:DeltaUnsupportedOperationsCheck.scala:119', 'org.apache.spark.sql.delta.DeltaUnsupportedOperationsCheck:$anonfun$apply$1$adapted:DeltaUnsupportedOperationsCheck.scala:52', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreach:TreeNode.scala:173', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreach$1:TreeNode.scala:174', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreach$1$adapted:TreeNode.scala:174', 'scala.collection.immutable.List:foreach:List.scala:392', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreach:TreeNode.scala:174', 'org.apache.spark.sql.delta.DeltaUnsupportedOperationsCheck:apply:DeltaUnsupportedOperationsCheck.scala:52', 'org.apache.spark.sql.delta.DeltaUnsupportedOperationsCheck:apply:DeltaUnsupportedOperationsCheck.scala:36', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis$46:CheckAnalysis.scala:699', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis$46$adapted:CheckAnalysis.scala:699', 'scala.collection.mutable.ResizableArray:foreach:ResizableArray.scala:62', 'scala.collection.mutable.ResizableArray:foreach$:ResizableArray.scala:55', 'scala.collection.mutable.ArrayBuffer:foreach:ArrayBuffer.scala:49', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis:CheckAnalysis.scala:699', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis$:CheckAnalysis.scala:90', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis:Analyzer.scala:155', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:176', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:228', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:173', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:73', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:111', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:143', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:143', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:73', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:71', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:63', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:98', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:96', 'org.apache.spark.sql.SparkSession:$anonfun$sql$1:SparkSession.scala:615', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:610', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:650', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:325'], sqlState=None, errorCode=0, errorMessage='Error running query: java.lang.NullPointerException'), operationHandle=None)"
     ]
    }
   ],
   "source": [
    "#cursor.execute(\"USE default\")\n",
    "cursor.execute('DESCRIBE TABLE clean.green_clean')\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "frequent-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cursor.execute('DROP DATABASE clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "victorian-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-airplane",
   "metadata": {},
   "source": [
    "Test out jovyan user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "representative-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "connection = hive.connect(username='jovyan', host=\"spark-thrift-server\", port=10000, auth='NOSASL')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spread-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('clean', 'green_clean', False)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('SHOW TABLES in clean')\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lightweight-performer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: java.lang.NullPointerException:36:35', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:361', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:249', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:278', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:44', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:78', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:62', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:44', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:42', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:43', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'sun.reflect.GeneratedMethodAccessor53:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1730', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy26:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:457', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:53', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*java.lang.NullPointerException:null:185:149', 'org.apache.spark.util.ClosureCleaner$:clean:ClosureCleaner.scala:359', 'org.apache.spark.util.ClosureCleaner$:clean:ClosureCleaner.scala:162', 'org.apache.spark.SparkContext:clean:SparkContext.scala:2465', 'org.apache.spark.rdd.RDD:$anonfun$map$1:RDD.scala:422', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:151', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:112', 'org.apache.spark.rdd.RDD:withScope:RDD.scala:414', 'org.apache.spark.rdd.RDD:map:RDD.scala:421', 'org.apache.spark.sql.delta.util.StateCache$CachedDS:<init>:StateCache.scala:52', 'org.apache.spark.sql.delta.util.StateCache:cacheDS:StateCache.scala:100', 'org.apache.spark.sql.delta.util.StateCache:cacheDS$:StateCache.scala:99', 'org.apache.spark.sql.delta.Snapshot:cacheDS:Snapshot.scala:55', 'org.apache.spark.sql.delta.Snapshot:cachedState$lzycompute:Snapshot.scala:117', 'org.apache.spark.sql.delta.Snapshot:cachedState:Snapshot.scala:116', 'org.apache.spark.sql.delta.Snapshot:state:Snapshot.scala:120', 'org.apache.spark.sql.delta.Snapshot:$anonfun$computedState$1:Snapshot.scala:140', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withJobDescription:DeltaProgressReporter.scala:53', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withStatusCode:DeltaProgressReporter.scala:32', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withStatusCode$:DeltaProgressReporter.scala:27', 'org.apache.spark.sql.delta.Snapshot:withStatusCode:Snapshot.scala:55', 'org.apache.spark.sql.delta.Snapshot:computedState$lzycompute:Snapshot.scala:137', 'org.apache.spark.sql.delta.Snapshot:computedState:Snapshot.scala:136', 'org.apache.spark.sql.delta.Snapshot:metadata:Snapshot.scala:179', 'org.apache.spark.sql.delta.Snapshot:toString:Snapshot.scala:290', 'java.lang.String:valueOf:String.java:2994', 'java.lang.StringBuilder:append:StringBuilder.java:131', 'org.apache.spark.sql.delta.Snapshot:$anonfun$new$1:Snapshot.scala:293', 'org.apache.spark.sql.delta.Snapshot:$anonfun$logInfo$1:Snapshot.scala:270', 'org.apache.spark.internal.Logging:logInfo:Logging.scala:57', 'org.apache.spark.internal.Logging:logInfo$:Logging.scala:56', 'org.apache.spark.sql.delta.Snapshot:logInfo:Snapshot.scala:270', 'org.apache.spark.sql.delta.Snapshot:<init>:Snapshot.scala:293', 'org.apache.spark.sql.delta.SnapshotManagement:createSnapshot:SnapshotManagement.scala:223', 'org.apache.spark.sql.delta.SnapshotManagement:createSnapshot$:SnapshotManagement.scala:211', 'org.apache.spark.sql.delta.DeltaLog:createSnapshot:DeltaLog.scala:59', 'org.apache.spark.sql.delta.SnapshotManagement:getSnapshotAtInit:SnapshotManagement.scala:195', 'org.apache.spark.sql.delta.SnapshotManagement:getSnapshotAtInit$:SnapshotManagement.scala:186', 'org.apache.spark.sql.delta.DeltaLog:getSnapshotAtInit:DeltaLog.scala:59', 'org.apache.spark.sql.delta.SnapshotManagement:$init$:SnapshotManagement.scala:49', 'org.apache.spark.sql.delta.DeltaLog:<init>:DeltaLog.scala:63', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$3:DeltaLog.scala:467', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$2:DeltaLog.scala:467', 'com.databricks.spark.util.DatabricksLogging:recordOperation:DatabricksLogging.scala:77', 'com.databricks.spark.util.DatabricksLogging:recordOperation$:DatabricksLogging.scala:67', 'org.apache.spark.sql.delta.DeltaLog$:recordOperation:DeltaLog.scala:367', 'org.apache.spark.sql.delta.metering.DeltaLogging:recordDeltaOperation:DeltaLogging.scala:106', 'org.apache.spark.sql.delta.metering.DeltaLogging:recordDeltaOperation$:DeltaLogging.scala:91', 'org.apache.spark.sql.delta.DeltaLog$:recordDeltaOperation:DeltaLog.scala:367', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$1:DeltaLog.scala:466', 'com.google.common.cache.LocalCache$LocalManualCache$1:load:LocalCache.java:4792', 'com.google.common.cache.LocalCache$LoadingValueReference:loadFuture:LocalCache.java:3599', 'com.google.common.cache.LocalCache$Segment:loadSync:LocalCache.java:2379', 'com.google.common.cache.LocalCache$Segment:lockedGetOrLoad:LocalCache.java:2342', 'com.google.common.cache.LocalCache$Segment:get:LocalCache.java:2257', 'com.google.common.cache.LocalCache:get:LocalCache.java:4000', 'com.google.common.cache.LocalCache$LocalManualCache:get:LocalCache.java:4789', 'org.apache.spark.sql.delta.DeltaLog$:apply:DeltaLog.scala:464', 'org.apache.spark.sql.delta.DeltaLog$:forTable:DeltaLog.scala:401', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:deltaLog$lzycompute:DeltaTableV2.scala:73', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:deltaLog:DeltaTableV2.scala:73', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:$anonfun$snapshot$3:DeltaTableV2.scala:100', 'scala.Option:getOrElse:Option.scala:189', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:snapshot$lzycompute:DeltaTableV2.scala:100', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:snapshot:DeltaTableV2.scala:89', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:tableSchema$lzycompute:DeltaTableV2.scala:104', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:tableSchema:DeltaTableV2.scala:103', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:schema:DeltaTableV2.scala:106', 'org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$:create:DataSourceV2Relation.scala:176', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$lookupRelation$1:Analyzer.scala:1201', 'scala.Option:map:Option.scala:230', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:loaded$lzycompute$1:Analyzer.scala:1173', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:loaded$1:Analyzer.scala:1173', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$lookupRelation$3:Analyzer.scala:1211', 'scala.Option:orElse:Option.scala:447', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation:Analyzer.scala:1210', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10:applyOrElse:Analyzer.scala:1135', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10:applyOrElse:Analyzer.scala:1102', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$3:AnalysisHelper.scala:90', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:TreeNode.scala:73', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$1:AnalysisHelper.scala:90', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp:AnalysisHelper.scala:86', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp$:AnalysisHelper.scala:84', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUp:LogicalPlan.scala:29', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$2:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$mapChildren$1:TreeNode.scala:407', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapProductIterator:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:405', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:358', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$1:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp:AnalysisHelper.scala:86', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp$:AnalysisHelper.scala:84', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUp:LogicalPlan.scala:29', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$2:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$mapChildren$1:TreeNode.scala:407', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapProductIterator:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:405', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:358', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$1:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp:AnalysisHelper.scala:86', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp$:AnalysisHelper.scala:84', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUp:LogicalPlan.scala:29', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$2:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$mapChildren$1:TreeNode.scala:407', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapProductIterator:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:405', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:358', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$1:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp:AnalysisHelper.scala:86', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp$:AnalysisHelper.scala:84', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUp:LogicalPlan.scala:29', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1102', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1070', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:216', 'scala.collection.LinearSeqOptimized:foldLeft:LinearSeqOptimized.scala:126', 'scala.collection.LinearSeqOptimized:foldLeft$:LinearSeqOptimized.scala:122', 'scala.collection.immutable.List:foldLeft:List.scala:89', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:213', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:205', 'scala.collection.immutable.List:foreach:List.scala:392', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:205', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:196', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:190', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:155', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$executeAndTrack$1:RuleExecutor.scala:183', 'org.apache.spark.sql.catalyst.QueryPlanningTracker$:withTracker:QueryPlanningTracker.scala:88', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:executeAndTrack:RuleExecutor.scala:183', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:174', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:228', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:173', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:73', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:111', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:143', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:143', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:73', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:71', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:63', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:98', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:96', 'org.apache.spark.sql.SparkSession:$anonfun$sql$1:SparkSession.scala:615', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:610', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:650', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:325'], sqlState=None, errorCode=0, errorMessage='Error running query: java.lang.NullPointerException'), operationHandle=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-174a22685296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select * from clean.green_clean limit 10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/spark/lib/python3.8/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecuteStatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0m_check_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operationHandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperationHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/spark/lib/python3.8/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusCode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTStatusCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: java.lang.NullPointerException:36:35', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:361', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:249', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:278', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:44', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:78', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:62', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:44', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:42', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:43', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'sun.reflect.GeneratedMethodAccessor53:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1730', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy26:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:457', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:53', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*java.lang.NullPointerException:null:185:149', 'org.apache.spark.util.ClosureCleaner$:clean:ClosureCleaner.scala:359', 'org.apache.spark.util.ClosureCleaner$:clean:ClosureCleaner.scala:162', 'org.apache.spark.SparkContext:clean:SparkContext.scala:2465', 'org.apache.spark.rdd.RDD:$anonfun$map$1:RDD.scala:422', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:151', 'org.apache.spark.rdd.RDDOperationScope$:withScope:RDDOperationScope.scala:112', 'org.apache.spark.rdd.RDD:withScope:RDD.scala:414', 'org.apache.spark.rdd.RDD:map:RDD.scala:421', 'org.apache.spark.sql.delta.util.StateCache$CachedDS:<init>:StateCache.scala:52', 'org.apache.spark.sql.delta.util.StateCache:cacheDS:StateCache.scala:100', 'org.apache.spark.sql.delta.util.StateCache:cacheDS$:StateCache.scala:99', 'org.apache.spark.sql.delta.Snapshot:cacheDS:Snapshot.scala:55', 'org.apache.spark.sql.delta.Snapshot:cachedState$lzycompute:Snapshot.scala:117', 'org.apache.spark.sql.delta.Snapshot:cachedState:Snapshot.scala:116', 'org.apache.spark.sql.delta.Snapshot:state:Snapshot.scala:120', 'org.apache.spark.sql.delta.Snapshot:$anonfun$computedState$1:Snapshot.scala:140', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withJobDescription:DeltaProgressReporter.scala:53', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withStatusCode:DeltaProgressReporter.scala:32', 'org.apache.spark.sql.delta.util.DeltaProgressReporter:withStatusCode$:DeltaProgressReporter.scala:27', 'org.apache.spark.sql.delta.Snapshot:withStatusCode:Snapshot.scala:55', 'org.apache.spark.sql.delta.Snapshot:computedState$lzycompute:Snapshot.scala:137', 'org.apache.spark.sql.delta.Snapshot:computedState:Snapshot.scala:136', 'org.apache.spark.sql.delta.Snapshot:metadata:Snapshot.scala:179', 'org.apache.spark.sql.delta.Snapshot:toString:Snapshot.scala:290', 'java.lang.String:valueOf:String.java:2994', 'java.lang.StringBuilder:append:StringBuilder.java:131', 'org.apache.spark.sql.delta.Snapshot:$anonfun$new$1:Snapshot.scala:293', 'org.apache.spark.sql.delta.Snapshot:$anonfun$logInfo$1:Snapshot.scala:270', 'org.apache.spark.internal.Logging:logInfo:Logging.scala:57', 'org.apache.spark.internal.Logging:logInfo$:Logging.scala:56', 'org.apache.spark.sql.delta.Snapshot:logInfo:Snapshot.scala:270', 'org.apache.spark.sql.delta.Snapshot:<init>:Snapshot.scala:293', 'org.apache.spark.sql.delta.SnapshotManagement:createSnapshot:SnapshotManagement.scala:223', 'org.apache.spark.sql.delta.SnapshotManagement:createSnapshot$:SnapshotManagement.scala:211', 'org.apache.spark.sql.delta.DeltaLog:createSnapshot:DeltaLog.scala:59', 'org.apache.spark.sql.delta.SnapshotManagement:getSnapshotAtInit:SnapshotManagement.scala:195', 'org.apache.spark.sql.delta.SnapshotManagement:getSnapshotAtInit$:SnapshotManagement.scala:186', 'org.apache.spark.sql.delta.DeltaLog:getSnapshotAtInit:DeltaLog.scala:59', 'org.apache.spark.sql.delta.SnapshotManagement:$init$:SnapshotManagement.scala:49', 'org.apache.spark.sql.delta.DeltaLog:<init>:DeltaLog.scala:63', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$3:DeltaLog.scala:467', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$2:DeltaLog.scala:467', 'com.databricks.spark.util.DatabricksLogging:recordOperation:DatabricksLogging.scala:77', 'com.databricks.spark.util.DatabricksLogging:recordOperation$:DatabricksLogging.scala:67', 'org.apache.spark.sql.delta.DeltaLog$:recordOperation:DeltaLog.scala:367', 'org.apache.spark.sql.delta.metering.DeltaLogging:recordDeltaOperation:DeltaLogging.scala:106', 'org.apache.spark.sql.delta.metering.DeltaLogging:recordDeltaOperation$:DeltaLogging.scala:91', 'org.apache.spark.sql.delta.DeltaLog$:recordDeltaOperation:DeltaLog.scala:367', 'org.apache.spark.sql.delta.DeltaLog$:$anonfun$apply$1:DeltaLog.scala:466', 'com.google.common.cache.LocalCache$LocalManualCache$1:load:LocalCache.java:4792', 'com.google.common.cache.LocalCache$LoadingValueReference:loadFuture:LocalCache.java:3599', 'com.google.common.cache.LocalCache$Segment:loadSync:LocalCache.java:2379', 'com.google.common.cache.LocalCache$Segment:lockedGetOrLoad:LocalCache.java:2342', 'com.google.common.cache.LocalCache$Segment:get:LocalCache.java:2257', 'com.google.common.cache.LocalCache:get:LocalCache.java:4000', 'com.google.common.cache.LocalCache$LocalManualCache:get:LocalCache.java:4789', 'org.apache.spark.sql.delta.DeltaLog$:apply:DeltaLog.scala:464', 'org.apache.spark.sql.delta.DeltaLog$:forTable:DeltaLog.scala:401', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:deltaLog$lzycompute:DeltaTableV2.scala:73', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:deltaLog:DeltaTableV2.scala:73', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:$anonfun$snapshot$3:DeltaTableV2.scala:100', 'scala.Option:getOrElse:Option.scala:189', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:snapshot$lzycompute:DeltaTableV2.scala:100', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:snapshot:DeltaTableV2.scala:89', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:tableSchema$lzycompute:DeltaTableV2.scala:104', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:tableSchema:DeltaTableV2.scala:103', 'org.apache.spark.sql.delta.catalog.DeltaTableV2:schema:DeltaTableV2.scala:106', 'org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$:create:DataSourceV2Relation.scala:176', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$lookupRelation$1:Analyzer.scala:1201', 'scala.Option:map:Option.scala:230', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:loaded$lzycompute$1:Analyzer.scala:1173', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:loaded$1:Analyzer.scala:1173', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$lookupRelation$3:Analyzer.scala:1211', 'scala.Option:orElse:Option.scala:447', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation:Analyzer.scala:1210', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10:applyOrElse:Analyzer.scala:1135', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10:applyOrElse:Analyzer.scala:1102', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$3:AnalysisHelper.scala:90', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:TreeNode.scala:73', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$1:AnalysisHelper.scala:90', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp:AnalysisHelper.scala:86', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp$:AnalysisHelper.scala:84', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUp:LogicalPlan.scala:29', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$2:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$mapChildren$1:TreeNode.scala:407', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapProductIterator:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:405', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:358', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$1:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp:AnalysisHelper.scala:86', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp$:AnalysisHelper.scala:84', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUp:LogicalPlan.scala:29', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$2:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$mapChildren$1:TreeNode.scala:407', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapProductIterator:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:405', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:358', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$1:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp:AnalysisHelper.scala:86', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp$:AnalysisHelper.scala:84', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUp:LogicalPlan.scala:29', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$2:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$mapChildren$1:TreeNode.scala:407', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapProductIterator:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:405', 'org.apache.spark.sql.catalyst.trees.TreeNode:mapChildren:TreeNode.scala:358', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUp$1:AnalysisHelper.scala:87', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:221', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp:AnalysisHelper.scala:86', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUp$:AnalysisHelper.scala:84', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUp:LogicalPlan.scala:29', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1102', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1070', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:216', 'scala.collection.LinearSeqOptimized:foldLeft:LinearSeqOptimized.scala:126', 'scala.collection.LinearSeqOptimized:foldLeft$:LinearSeqOptimized.scala:122', 'scala.collection.immutable.List:foldLeft:List.scala:89', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:213', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:205', 'scala.collection.immutable.List:foreach:List.scala:392', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:205', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:196', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:190', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:155', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$executeAndTrack$1:RuleExecutor.scala:183', 'org.apache.spark.sql.catalyst.QueryPlanningTracker$:withTracker:QueryPlanningTracker.scala:88', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:executeAndTrack:RuleExecutor.scala:183', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:174', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:228', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:173', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:73', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:111', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:143', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:143', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:73', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:71', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:63', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:98', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:96', 'org.apache.spark.sql.SparkSession:$anonfun$sql$1:SparkSession.scala:615', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:772', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:610', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:650', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:325'], sqlState=None, errorCode=0, errorMessage='Error running query: java.lang.NullPointerException'), operationHandle=None)"
     ]
    }
   ],
   "source": [
    "cursor.execute('select * from clean.green_clean limit 10')\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fresh-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-characterization",
   "metadata": {},
   "source": [
    "Further tests with pandas as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "productive-batman",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-bc46e918ad0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"select * from clean.green_clean limit 10\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"select * from clean.green_clean limit 10\"\n",
    "data = pd.read_sql(query, con=connection)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-saudi",
   "metadata": {},
   "source": [
    "# SQLAlchemy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acceptable-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "host_server = 'spark-thrift-server'\n",
    "port = '10000'\n",
    "database = 'default'\n",
    "conn = f'hive://{host_server}:{port}/{database}'\n",
    "engine = create_engine(conn, connect_args={'auth': 'NOSASL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "optimum-cincinnati",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('default',)\n"
     ]
    }
   ],
   "source": [
    "query = \"show databases\"\n",
    "con = engine.connect()\n",
    "\n",
    "result = con.execute(query)\n",
    "\n",
    "for line in result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "crude-airport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('default', 'green_clean', False)\n",
      "('default', 'green_merged', False)\n",
      "('default', 'green_taxi_2015_h1', False)\n",
      "('default', 'green_taxi_pre2015', False)\n",
      "('default', 'yellow_taxi_pre2015', False)\n"
     ]
    }
   ],
   "source": [
    "query = \"show tables\"\n",
    "con = engine.connect()\n",
    "\n",
    "result = con.execute(query)\n",
    "\n",
    "for line in result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "roman-brush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('default',)\n"
     ]
    }
   ],
   "source": [
    "query = \"show schemas\"\n",
    "con = engine.connect()\n",
    "\n",
    "result = con.execute(query)\n",
    "\n",
    "for line in result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "royal-browser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vendor_id', 'string', '')\n",
      "('pickup_datetime', 'timestamp', '')\n",
      "('dropoff_datetime', 'timestamp', '')\n",
      "('store_and_fwd_flag', 'string', '')\n",
      "('rate_code_id', 'int', '')\n",
      "('pickup_longitude', 'float', '')\n",
      "('pickup_latitude', 'float', '')\n",
      "('dropoff_longitude', 'float', '')\n",
      "('dropoff_latitude', 'float', '')\n",
      "('passenger_count', 'int', '')\n",
      "('trip_distance', 'float', '')\n",
      "('fare_amount', 'float', '')\n",
      "('extra', 'float', '')\n",
      "('mta_tax', 'float', '')\n",
      "('tip_amount', 'float', '')\n",
      "('tolls_amount', 'float', '')\n",
      "('ehail_fee', 'float', '')\n",
      "('total_amount', 'float', '')\n",
      "('payment_type', 'int', '')\n",
      "('trip_type', 'int', '')\n",
      "('improvement_surcharge', 'float', '')\n",
      "('', '', '')\n",
      "('# Partitioning', '', '')\n",
      "('Not partitioned', '', '')\n"
     ]
    }
   ],
   "source": [
    "query = \"describe table green_clean\"\n",
    "con = engine.connect()\n",
    "\n",
    "result = con.execute(query)\n",
    "\n",
    "for line in result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "editorial-logging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2', '2014-02-01 00:00:00', '2014-02-01 23:25:08', 'N', '2', '0', '0', '-73.912254333496094', '40.753227233886719', '2', '.89', '52', '0', '0.5', '0', '0', None, '52.5', '2', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 20:17:35', 'N', '1', '0', '0', '-73.960289001464844', '40.761558532714844', '1', '2.72', '11', '0.5', '0.5', '0.75', '0', None, '12.75', '1', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 14:27:37', 'N', '1', '0', '0', '-73.937232971191406', '40.758316040039063', '1', '.00', '2.5', '0', '0.5', '0', '0', None, '3', '1', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 01:07:26', 'N', '1', '0', '0', '-73.947052001953125', '40.683628082275391', '1', '3.21', '12.5', '0.5', '0.5', '3.9', '0', None, '17.4', '1', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 04:04:40', 'N', '1', '0', '0', '-73.976493835449219', '40.788700103759766', '1', '9.71', '31', '0.5', '0.5', '6.3', '0', None, '38.3', '1', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 11:24:18', 'N', '1', '0', '0', '0', '0', '1', '3.05', '13.5', '0', '0.5', '2.7', '0', None, '16.7', '1', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 20:43:09', 'N', '1', '0', '0', '-73.995849609375', '40.764453887939453', '1', '4.36', '15', '0.5', '0.5', '2', '0', None, '18', '1', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 21:25:35', 'N', '1', '0', '0', '0', '0', '1', '5.11', '19', '0.5', '0.5', '4.88', '0', None, '24.88', '1', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 07:36:10', 'N', '1', '0', '0', '0', '0', '1', '10.33', '29', '0', '0.5', '8.58', '5.33', None, '43.41', '1', None)\n",
      "('2', '2014-02-01 00:00:00', '2014-02-01 21:22:18', 'N', '1', '0', '0', '-73.830276489257813', '40.893222808837891', '1', '10.86', '31.5', '0.5', '0.5', '0', '0', None, '32.5', '1', None)\n"
     ]
    }
   ],
   "source": [
    "query = \"select * from green_taxi_pre2015 limit 10\"\n",
    "con = engine.connect()\n",
    "\n",
    "result = con.execute(query)\n",
    "\n",
    "for line in result:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
