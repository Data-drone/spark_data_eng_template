version: '3.7'
services:
  metastore_db:
    image: postgres:10.5
    hostname: metastore
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_HOST: ${POSTGRES_HOST}
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U metastore" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: 
        - ml_platform
  hive_metastore:
    hostname: hive_metastore
    build:
      context: ./hivemetastore
    depends_on:
      metastore_db:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    networks: 
      - ml_platform      
  minio:
    image: minio/minio
    ports:
      - "9001:9000"
    volumes:
      - /mnt/38390e7f-c8b3-4b79-8750-eff41b386f03/ml_platform_data:/data 
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    hostname: minio
    entrypoint: sh
    command: -c 'mkdir -p /data/storage && /usr/bin/minio server /data'  
    networks: 
      - ml_platform
  spark-master:
    hostname: spark-master
    image: datadrone/spark-master:3.1.1-hadoop3.2-rapids
  #  build:
  #    context: ./spark_ext_metastore
  #    dockerfile: Dockerfile_Master
    ports:
      - "9080:8080"
      - "7077:7077"
    links:
      - hive_metastore
    environment:
      SPARK_LOCAL_IP: "spark-master"
      PYSPARK_PYTHON: /usr/bin/python3
      PYSPARK_DRIVER_PYTHON: /usr/bin/python3
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    networks: 
      - ml_platform

  spark-worker-1:
    image: datadrone/spark-worker:3.1.1-hadoop3.2-rapids
  #  build:
  #    context: ./spark_ext_metastore
  #    dockerfile: Dockerfile_Worker
    depends_on:
      - spark-master
    ports:
      - "4040:4040"
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      SPARK_WORKER_CORES: 16
      SPARK_WORKER_MEMORY: 32G
      SPARK_DRIVER_MEMORY: 128m
      SPARK_EXECUTOR_MEMORY: 256m
      SPARK_WORKER_OPTS: -Dspark.worker.resource.gpu.amount=2 -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh
      PYSPARK_PYTHON: /usr/bin/python3
      PYSPARK_DRIVER_PYTHON: /usr/bin/python3
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0', '1']
            capabilities: [gpu, utility, compute]
    links:
        - hive_metastore
    networks:
      - ml_platform

  spark-thrift-server:
    #image: datadrone/spark-submit:3.1.1-hadoop3.2-rapids
    build:
      context: ./spark_thrift_server
    ports:
      - "10000:10000"
    depends_on:
      - spark-worker-1
    environment:
      JAVA_HOME: /usr
      SPARK_MASTER: spark://spark-master:7077
      PYSPARK_PYTHON: /usr/bin/python3
      PYSPARK_DRIVER_PYTHON: /usr/bin/python3
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      SPARK_APPLICATION_MAIN_CLASS: org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
      SPARK_MASTER_URL: spark://spark-master:7077
    networks:
      - ml_platform

  spark-notebook:
    image: datadrone/spark_notebook:3.1.1-hadoop3.2
  #  build:
  #    context: ./spark_ext_metastore
  #    dockerfile: Dockerfile_NB
    ports:
      - "9494:8888"
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JAVA_HOME: /usr
      SPARK_MASTER: "spark://spark-master:7077"
      PYSPARK_PYTHON: /usr/bin/python3
      PYSPARK_DRIVER_PYTHON: /usr/bin/python3
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
    ipc: host  
    links:
        - hive_metastore
    networks:
      - ml_platform

networks:
  ml_platform:
    external: true